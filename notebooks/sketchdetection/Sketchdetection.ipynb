{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc267fb3-e636-4757-b1b5-8603015f5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d19f734-245f-4cd7-804f-0a66861029ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_mask(dataset, idx):\n",
    "    image, target_dict = dataset[idx]\n",
    "    image = image\n",
    "    mask = target_dict['masks'][0].numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axs[0].imshow(image, cmap='gray')\n",
    "    axs[0].set_title('Image')\n",
    "    axs[1].imshow(mask, cmap='gray')\n",
    "    axs[1].set_title('Mask')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8b3380-1bd7-449c-ae21-c60cb20d91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_with_bbox(dataset, idx):\n",
    "    image, target_dict = dataset[idx]\n",
    "    bbox = target_dict['boxes'][0].numpy()\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, edgecolor='red', linewidth=2))\n",
    "    plt.title('Image with Bounding Box')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce772330-3dc2-4b05-b1ad-98bf4fd53b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f0334d-8e95-438f-be50-190dbffc66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchDataset(Dataset):\n",
    "    def __init__(self, image_dir, target_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.transform = transform\n",
    "       \n",
    "        self.images = sorted([img for img in os.listdir(image_dir) if img.endswith('.png')])\n",
    "        self.targets = sorted([tgt for tgt in os.listdir(target_dir) if tgt.endswith('.png')])\n",
    "\n",
    "        assert len(self.images) == len(self.targets), \"The number of images and targets must be the same\"\n",
    "        assert all(img.split('.')[0] == tgt.split('.')[0] for img, tgt in zip(self.images, self.targets)), \"Image and target filenames must match\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "\n",
    "        target_path = os.path.join(self.target_dir, self.images[idx])\n",
    "        \n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = image / 255\n",
    "        \n",
    "        \n",
    "        target = cv2.imread(target_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if image is None or target is None:\n",
    "            raise RuntimeError(f\"Failed to load image or target at index {idx}\")\n",
    "        \n",
    "        \n",
    "        target[target != 0] = 1  # Convert to binary mask\n",
    "\n",
    "        bbox = self.get_bounding_box(target)\n",
    "\n",
    "        # Convert bbox from (xmin, ymin, xmax, ymax) to [N, 4] tensor\n",
    "        boxes = torch.as_tensor([bbox], dtype=torch.float32)\n",
    "\n",
    "        # There is only one class (mask)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "\n",
    "        # Convert mask to [N, H, W] tensor\n",
    "        masks = torch.as_tensor(target, dtype=torch.uint8)\n",
    "        masks = masks.unsqueeze(0)  # Add an extra dimension for N\n",
    "\n",
    "        target_dict = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bounding_box(mask):\n",
    "        rows = np.any(mask, axis=1)\n",
    "        cols = np.any(mask, axis=0)\n",
    "        ymin, ymax = np.where(rows)[0][[0, -1]]\n",
    "        xmin, xmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "        return (xmin, ymin, xmax, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1ed8bbc-09bb-474f-93ea-6d63fc920f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"../../datasets/sketch-parse/images\"\n",
    "TARGET_DIR = \"../../datasets/sketch-parse/masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81438952-ace7-478b-8029-2453fb279cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SketchDataset(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    target_dir=TARGET_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3445717-0f72-4267-9061-6c7280d4cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "test_size = dataset_size-train_size\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07cdb539-00f4-47da-806d-5ce94dfda5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maskrcnn_resnet50_fpn():\n",
    "    num_classes = 2 #Background and object\n",
    "    \n",
    "    # Load a model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='COCO_V1')\n",
    "\n",
    "    # Replace the classifier with a new one for our number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Replace the mask predictor with a new one for our number of classes\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76dd3147-3f73-4a08-bcb2-ca4d3b51b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb6199d-2855-47cf-8394-e7143806aa20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_maskrcnn_resnet50_fpn().to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d340e1-f073-4953-a762-4cdbc721b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005,momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9902181c-3762-4fb9-b357-fa52bf996f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 0.6547335605056299 | Test Loss: 0.3278859766324361\n",
      "Epoch [2/100], Training Loss: 0.2743396882204651 | Test Loss: 0.23017881910006205\n",
      "Epoch [3/100], Training Loss: 0.22174706529675192 | Test Loss: 0.20594605505466462\n",
      "Epoch [4/100], Training Loss: 0.1858733260648788 | Test Loss: 0.18831702669461567\n",
      "Epoch [5/100], Training Loss: 0.17876692047353424 | Test Loss: 0.18608784159024558\n",
      "Epoch [6/100], Training Loss: 0.17492632033852484 | Test Loss: 0.18537440657615661\n",
      "Epoch [7/100], Training Loss: 0.17038921934331772 | Test Loss: 0.18532869617144268\n",
      "Epoch [8/100], Training Loss: 0.17022478980997394 | Test Loss: 0.18530551294485728\n",
      "Epoch [9/100], Training Loss: 0.17019036257198092 | Test Loss: 0.18482691089312236\n",
      "Epoch [10/100], Training Loss: 0.17008321254239606 | Test Loss: 0.18455503424008687\n",
      "Epoch [11/100], Training Loss: 0.1695884486843396 | Test Loss: 0.18515389502048493\n",
      "Epoch [12/100], Training Loss: 0.16989651697047184 | Test Loss: 0.18494991600513458\n",
      "Epoch [13/100], Training Loss: 0.16931107235437184 | Test Loss: 0.1850943461060524\n",
      "Epoch [14/100], Training Loss: 0.1701188277819253 | Test Loss: 0.18492118239402772\n",
      "Epoch [15/100], Training Loss: 0.16925207008688437 | Test Loss: 0.18493728796641032\n",
      "Epoch [16/100], Training Loss: 0.16983209761408713 | Test Loss: 0.185359578927358\n",
      "Epoch [17/100], Training Loss: 0.16917441346052753 | Test Loss: 0.18472224136193593\n",
      "Epoch [18/100], Training Loss: 0.16984541679737886 | Test Loss: 0.18509773711363475\n",
      "Epoch [19/100], Training Loss: 0.16954179814440667 | Test Loss: 0.18496706167856852\n",
      "Epoch [20/100], Training Loss: 0.1696923793309686 | Test Loss: 0.1850615986188253\n",
      "Epoch [21/100], Training Loss: 0.16937285084600393 | Test Loss: 0.1850307685136795\n",
      "Epoch [22/100], Training Loss: 0.16955239790884746 | Test Loss: 0.18507625778516135\n",
      "Epoch [23/100], Training Loss: 0.16979847745054719 | Test Loss: 0.18541409532229106\n",
      "Epoch [24/100], Training Loss: 0.16952432765264733 | Test Loss: 0.18525397916634878\n",
      "Epoch [25/100], Training Loss: 0.16987916741067963 | Test Loss: 0.18529524167378744\n",
      "Epoch [26/100], Training Loss: 0.1695732331344847 | Test Loss: 0.1847197949886322\n",
      "Epoch [27/100], Training Loss: 0.1694715658708804 | Test Loss: 0.18500885844230652\n",
      "Epoch [28/100], Training Loss: 0.16980468107096722 | Test Loss: 0.1852929095427195\n",
      "Epoch [29/100], Training Loss: 0.16975561249462856 | Test Loss: 0.18530517836411795\n",
      "Epoch [30/100], Training Loss: 0.16948219991660532 | Test Loss: 0.18505299607912698\n",
      "Epoch [31/100], Training Loss: 0.16974117576731423 | Test Loss: 0.18532038648923238\n",
      "Epoch [32/100], Training Loss: 0.16958677738560418 | Test Loss: 0.18511489828427632\n",
      "Epoch [33/100], Training Loss: 0.1698302219196551 | Test Loss: 0.18494517147541045\n",
      "Epoch [34/100], Training Loss: 0.16994407355268568 | Test Loss: 0.18510111192862191\n",
      "Epoch [35/100], Training Loss: 0.169385056102896 | Test Loss: 0.18515299042065939\n",
      "Epoch [36/100], Training Loss: 0.1696315929724302 | Test Loss: 0.18523948828379314\n",
      "Epoch [37/100], Training Loss: 0.16929265934263352 | Test Loss: 0.18532485087712605\n",
      "Epoch [38/100], Training Loss: 0.16966913575898704 | Test Loss: 0.18526331782341005\n",
      "Epoch [39/100], Training Loss: 0.16981136368189245 | Test Loss: 0.1854148906469345\n",
      "Epoch [40/100], Training Loss: 0.1697612114982798 | Test Loss: 0.1854190085331599\n",
      "Epoch [41/100], Training Loss: 0.16995859981616798 | Test Loss: 0.18511665503184\n",
      "Epoch [42/100], Training Loss: 0.1697095585610136 | Test Loss: 0.18501459419727326\n",
      "Epoch [43/100], Training Loss: 0.16977490170325846 | Test Loss: 0.18464110136032105\n",
      "Epoch [44/100], Training Loss: 0.17028429845854037 | Test Loss: 0.18522901674111683\n",
      "Epoch [45/100], Training Loss: 0.16968962954992503 | Test Loss: 0.18509022066990535\n",
      "Epoch [46/100], Training Loss: 0.16982079491105384 | Test Loss: 0.18507565677165985\n",
      "Epoch [47/100], Training Loss: 0.16923258569888297 | Test Loss: 0.18514223198095958\n",
      "Epoch [48/100], Training Loss: 0.16979858632377118 | Test Loss: 0.18530527333418528\n",
      "Epoch [49/100], Training Loss: 0.16924573997886194 | Test Loss: 0.18520197451114653\n",
      "Epoch [50/100], Training Loss: 0.16958708406528297 | Test Loss: 0.18538642525672913\n",
      "Epoch [51/100], Training Loss: 0.16975602460217615 | Test Loss: 0.18525757431983947\n",
      "Epoch [52/100], Training Loss: 0.1692446804029404 | Test Loss: 0.18561634759108225\n",
      "Epoch [53/100], Training Loss: 0.16934910619017707 | Test Loss: 0.18534499883651734\n",
      "Epoch [54/100], Training Loss: 0.1697547350487957 | Test Loss: 0.18495426734288534\n",
      "Epoch [55/100], Training Loss: 0.16928690366145505 | Test Loss: 0.18529555638631184\n",
      "Epoch [56/100], Training Loss: 0.1696369807327414 | Test Loss: 0.18501493612925213\n",
      "Epoch [57/100], Training Loss: 0.16908254551921967 | Test Loss: 0.18467809001604715\n",
      "Epoch [58/100], Training Loss: 0.1695821723238581 | Test Loss: 0.18513138234615326\n",
      "Epoch [59/100], Training Loss: 0.1697199224047578 | Test Loss: 0.18518341143925984\n",
      "Epoch [60/100], Training Loss: 0.1692235596004249 | Test Loss: 0.18520091156164806\n",
      "Epoch [61/100], Training Loss: 0.16963811214431862 | Test Loss: 0.18503042777379353\n",
      "Epoch [62/100], Training Loss: 0.16918165930089235 | Test Loss: 0.1846849004427592\n",
      "Epoch [63/100], Training Loss: 0.1692992357074181 | Test Loss: 0.18495049814383188\n",
      "Epoch [64/100], Training Loss: 0.16968036076926082 | Test Loss: 0.1849990377823512\n",
      "Epoch [65/100], Training Loss: 0.1696113876007885 | Test Loss: 0.18517908612887066\n",
      "Epoch [66/100], Training Loss: 0.16943730767062634 | Test Loss: 0.18529799669981004\n",
      "Epoch [67/100], Training Loss: 0.1697762428938998 | Test Loss: 0.18521659711996713\n",
      "Epoch [68/100], Training Loss: 0.16953375741753274 | Test Loss: 0.18503350536028543\n",
      "Epoch [69/100], Training Loss: 0.16994352483680483 | Test Loss: 0.1853557050228119\n",
      "Epoch [70/100], Training Loss: 0.1694329246189553 | Test Loss: 0.18504254003365836\n",
      "Epoch [71/100], Training Loss: 0.16935534219693588 | Test Loss: 0.18521213740110398\n",
      "Epoch [72/100], Training Loss: 0.16948515956284682 | Test Loss: 0.18523369749387106\n",
      "Epoch [73/100], Training Loss: 0.16935366560096685 | Test Loss: 0.18507768432299296\n",
      "Epoch [74/100], Training Loss: 0.1697638866012496 | Test Loss: 0.18487883249918619\n",
      "Epoch [75/100], Training Loss: 0.16942780232326143 | Test Loss: 0.18476516783237457\n",
      "Epoch [76/100], Training Loss: 0.1697398908651633 | Test Loss: 0.1850412493944168\n",
      "Epoch [77/100], Training Loss: 0.16943292022612744 | Test Loss: 0.18492702464262645\n",
      "Epoch [78/100], Training Loss: 0.1704697472856224 | Test Loss: 0.18496793508529663\n",
      "Epoch [79/100], Training Loss: 0.1697332614419088 | Test Loss: 0.18514996528625488\n",
      "Epoch [80/100], Training Loss: 0.17018429792857584 | Test Loss: 0.18471932967503865\n",
      "Epoch [81/100], Training Loss: 0.17010233559877197 | Test Loss: 0.1851514450709025\n",
      "Epoch [82/100], Training Loss: 0.16993764503670566 | Test Loss: 0.18530592501163481\n",
      "Epoch [83/100], Training Loss: 0.1697236652233008 | Test Loss: 0.18540715058644613\n",
      "Epoch [84/100], Training Loss: 0.16956442840009756 | Test Loss: 0.1852095502614975\n",
      "Epoch [85/100], Training Loss: 0.169641546000635 | Test Loss: 0.18523825844128927\n",
      "Epoch [86/100], Training Loss: 0.16986250722339388 | Test Loss: 0.18527626156806945\n",
      "Epoch [87/100], Training Loss: 0.16937764686656137 | Test Loss: 0.18493723372618356\n",
      "Epoch [88/100], Training Loss: 0.16918122652121362 | Test Loss: 0.18518899897734323\n",
      "Epoch [89/100], Training Loss: 0.16920491962591347 | Test Loss: 0.18522811392943064\n",
      "Epoch [90/100], Training Loss: 0.16966916017794195 | Test Loss: 0.18531631350517272\n",
      "Epoch [91/100], Training Loss: 0.16996069997549057 | Test Loss: 0.1851798419157664\n",
      "Epoch [92/100], Training Loss: 0.1700802527238868 | Test Loss: 0.18528644353151322\n",
      "Epoch [93/100], Training Loss: 0.1696738700215527 | Test Loss: 0.18521091381708782\n",
      "Epoch [94/100], Training Loss: 0.16993807617984066 | Test Loss: 0.18528079152107238\n",
      "Epoch [95/100], Training Loss: 0.16960799870635732 | Test Loss: 0.18492737809816998\n",
      "Epoch [96/100], Training Loss: 0.17013051197191195 | Test Loss: 0.18505700012048085\n",
      "Epoch [97/100], Training Loss: 0.16984222829341888 | Test Loss: 0.18548821568489074\n",
      "Epoch [98/100], Training Loss: 0.16948159064860702 | Test Loss: 0.18485956609249116\n",
      "Epoch [99/100], Training Loss: 0.16965759786739515 | Test Loss: 0.1851953015724818\n",
      "Epoch [100/100], Training Loss: 0.16977230575732413 | Test Loss: 0.18499057630697885\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "if start_epoch > 0:\n",
    "    checkpoint = torch.load(f\"results/model_epoch_{start_epoch}.pth\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = list(torch.from_numpy(img.astype(np.float32)).unsqueeze(0).to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += losses.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_test_loss = 0\n",
    "        for images, targets in test_loader:\n",
    "            images = list(torch.from_numpy(img.astype(np.float32)).unsqueeze(0).to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            test_loss_dict = model(images, targets)\n",
    "            test_loss = sum(loss for loss in test_loss_dict.values())\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss} | Test Loss: {avg_test_loss}\")\n",
    "\n",
    "\n",
    "    if(epoch % 10 == 0):\n",
    "        state = {\n",
    "            'epoch': epoch + 1,  # next epoch number\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }   \n",
    "        torch.save(state, f'results/model_epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a721d6d3-5c4a-453f-8e38-9d18d9ea5b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
