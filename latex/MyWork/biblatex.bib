@article{eitz2012hdhso,
author={Eitz, Mathias and Hays, James and Alexa, Marc},
title={How Do Humans Sketch Objects?},
journal={ACM Trans. Graph. (Proc. SIGGRAPH)},
year={2012},
volume={31},
number={4},
pages = {44:1--44:10}
}

@inproceedings{10.1145/3469877.3490565,
author = {Huang, Zhanpeng and Han, Rui and Huang, Jianwen and Yin, Hao and Qin, Zipeng and Wang, Zibin},
title = {Automatically Generate Rigged Character from Single Image},
year = {2022},
isbn = {9781450386074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469877.3490565},
doi = {10.1145/3469877.3490565},
abstract = {Animation plays an important role in virtual reality and augmented reality applications. However, it requires great efforts for non-professional users to create animation assets. In this paper, we propose a systematic pipeline to generate ready-to-used characters from images for real-time animation without user intervention. Rather than per-pixel mapping or synthesis in image space using optical flow or generative models, we employ an approximate geometric embodiment to undertake 3D animation without large distortion. The geometry structure is generated from a type-agnostic character. A skeleton adaption is then adopted to guarantee semantic motion transfer to the geometry proxy. The generated character is compatible with standard 3D graphics engines and ready to use for real-time applications. Experiments show that our method works on various images (e.g. sketches, cartoons, and photos) of most object categories (e.g. human, animals, and non-creatures). We develop an AR demo to show its potential usage for fast prototyping.},
booktitle = {ACM Multimedia Asia},
articleno = {35},
numpages = {5},
keywords = {salient object detection, skeletion extraction, animation transfer, auto skinning, joint mapping},
location = {Gold Coast, Australia},
series = {MMAsia '21}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@mastersthesis{korpitsch-2023-sao,
  title =      "Semantic-Aware Animation of Hand-Drawn Characters",
  author =     "Thorsten Korpitsch",
  year =       "2023",
  abstract =   "In recent years there has been a lot of research in the area
               of edutainment, which facilitates effective learning
               processes by increasing the engagement of the learners.
               Guided visualisations, such as audio-guided museum tours or
               Augmented Reality-guided city tours, are one of the
               potential applications. Guided visualisations are a form of
               mental practice which traditionally involves verbal guidance
               that guides a user through a series of visualisations. With
               the technique of Augmented Reality, one can integrate
               additional information to guide users or embody verbal
               guidance with a virtual character, which enables an engaging
               experience. In this thesis, we aim to make a first step
               towards guided visualisation by introducing a hand-drawn
               character for instruction purposes. We especially focus on
               animation, since character animations are used in different
               applications, such as computer graphics, but can be hardly
               generated without certain pre-knowledge. Here, we present a
               novel pipeline for automatically generating believable
               movements for hand-drawn characters. The approach consists
               of five steps. (1) the hand-drawn character is detected from
               an input image, and (2) the sub-parts of the drawn
               character, such as the legs and thehead, are identified,
               respectively. (3) A bone skeleton for animation is extracted
               and augmented with the semantic information from the
               previous step. (4) Based on the augmented skeleton, we
               assign a super-class that the skeleton belongs to, i.e.,
               quadruped, flying or humanoid, and match the end-effectors
               of the skeleton to the end-effectors of the reference
               skeleton of the super-class. (5) Finally, we generate a
               triangular mesh from the input illustration. Once the
               matching reference skeleton and the hand-drawn character are
               overlayed, the character is animated and can attract users
               in different applications. To show the feasibility of our
               approach, we evaluate the proposed pipeline with a set of
               hand-drawn characters showing several well-articulate
               drawings.",
  pages =      "91",
  address =    "Favoritenstrasse 9-11/E193-02, A-1040 Vienna, Austria",
  school =     "Research Unit of Computer Graphics, Institute of Visual
               Computing and Human-Centered Technology, Faculty of
               Informatics, TU Wien",
  keywords =   "computer animation, character animation",
  URL =        "https://www.cg.tuwien.ac.at/research/publications/2023/korpitsch-2023-sao/",
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{10.1145/2897824.2925954,
author = {Sangkloy, Patsorn and Burnell, Nathan and Ham, Cusuh and Hays, James},
title = {The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925954},
doi = {10.1145/2897824.2925954},
abstract = {We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us fine-grained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both hand-crafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {119},
numpages = {12},
keywords = {sketch-based image retrieval, triplet network, siamese network, deep learning, image synthesis}
}

@misc{sarvadevabhatla2017sketchparse,
      title={SketchParse : Towards Rich Descriptions for Poorly Drawn Sketches using Multi-Task Hierarchical Deep Networks}, 
      author={Ravi Kiran Sarvadevabhatla and Isht Dwivedi and Abhijat Biswas and Sahil Manocha and R. Venkatesh Babu},
      year={2017},
      eprint={1709.01295},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{smith2023method,
      title={A Method for Animating Children's Drawings of the Human Figure}, 
      author={Harrison Jesse Smith and Qingyuan Zheng and Yifei Li and Somya Jain and Jessica K. Hodgins},
      year={2023},
      eprint={2303.12741},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{he2018mask,
      title={Mask R-CNN}, 
      author={Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick},
      year={2018},
      eprint={1703.06870},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{10.1145/3592788,
author = {Smith, Harrison Jesse and Zheng, Qingyuan and Li, Yifei and Jain, Somya and Hodgins, Jessica K.},
title = {A Method for Animating Children’s Drawings of the Human Figure},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592788},
doi = {10.1145/3592788},
abstract = {Children’s drawings have a wonderful inventiveness, creativity, and variety to them. We present a system that automatically animates children’s drawings of the human figure, is robust to the variance inherent in these depictions, and is simple and straightforward enough for anyone to use. We demonstrate the value and broad appeal of our approach by building and releasing the Animated Drawings Demo, a freely available public website that has been used by millions of people around the world. We present a set of experiments exploring the amount of training data needed for fine-tuning, as well as a perceptual study demonstrating the appeal of a novel twisted perspective retargeting technique. Finally, we introduce the Amateur Drawings Dataset, a first-of-its-kind annotated dataset, collected via the public demo, containing over 178,000 amateur drawings and corresponding user-accepted character bounding boxes, segmentation masks, and joint location annotations.},
journal = {ACM Trans. Graph.},
month = {jun},
articleno = {32},
numpages = {15},
keywords = {Skeletal animation, motion retargeting, motion stylization, 2D animation}
}